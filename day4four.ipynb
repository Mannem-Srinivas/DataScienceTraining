{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb64081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading the Breast Cancer Wisconsin dataset...\n",
      "\n",
      "Checking for class imbalance:\n",
      "target\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset split successfully.\n",
      "\n",
      "2. Scaling features...\n",
      "Features scaled successfully.\n",
      "\n",
      "3. Training models without imbalance handling...\n",
      "\n",
      "--- Logistic Regression Results ---\n",
      "Accuracy: 0.9883\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        64\n",
      "           1       0.99      0.99      0.99       107\n",
      "\n",
      "    accuracy                           0.99       171\n",
      "   macro avg       0.99      0.99      0.99       171\n",
      "weighted avg       0.99      0.99      0.99       171\n",
      "\n",
      "\n",
      "--- Decision Tree Results ---\n",
      "Accuracy: 0.9181\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89        64\n",
      "           1       0.93      0.93      0.93       107\n",
      "\n",
      "    accuracy                           0.92       171\n",
      "   macro avg       0.91      0.91      0.91       171\n",
      "weighted avg       0.92      0.92      0.92       171\n",
      "\n",
      "\n",
      "--- Random Forest Results ---\n",
      "Accuracy: 0.9357\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91        64\n",
      "           1       0.94      0.95      0.95       107\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.93      0.93      0.93       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n",
      "\n",
      "--- Gradient Boosting Results ---\n",
      "Accuracy: 0.9474\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93        64\n",
      "           1       0.95      0.97      0.96       107\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.95      0.94      0.94       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n",
      "\n",
      "--- SVM (C-Support Vector Classification) Results ---\n",
      "Accuracy: 0.9766\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        64\n",
      "           1       0.98      0.98      0.98       107\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.98      0.98       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n",
      "\n",
      "--- k-NN (k-Nearest Neighbors) Results ---\n",
      "Accuracy: 0.9591\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94        64\n",
      "           1       0.94      1.00      0.97       107\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.97      0.95      0.96       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n",
      "\n",
      "--- 4. Addressing Class Imbalance ---\n",
      "\n",
      "Using class_weight='balanced' parameter:\n",
      "\n",
      "--- Logistic Regression (Balanced) Results ---\n",
      "Accuracy: 0.9708\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96        64\n",
      "           1       0.99      0.96      0.98       107\n",
      "\n",
      "    accuracy                           0.97       171\n",
      "   macro avg       0.97      0.97      0.97       171\n",
      "weighted avg       0.97      0.97      0.97       171\n",
      "\n",
      "\n",
      "--- Random Forest (Balanced) Results ---\n",
      "Accuracy: 0.9415\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92        64\n",
      "           1       0.94      0.96      0.95       107\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.94      0.93      0.94       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n",
      "\n",
      "--- SVM (Balanced) Results ---\n",
      "Accuracy: 0.9766\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97        64\n",
      "           1       0.99      0.97      0.98       107\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.97      0.98      0.98       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n",
      "\n",
      "Skipping SMOTE implementation. Please install `imblearn` library to run this part.\n",
      "Run: pip install imbalanced-learn\n",
      "\n",
      "--- Process Complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# --- 1. Setup: Load Dataset ---\n",
    "print(\"1. Loading the Breast Cancer Wisconsin dataset...\")\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Check for class imbalance\n",
    "print(\"\\nChecking for class imbalance:\")\n",
    "print(y.value_counts())\n",
    "# You'll see that class 1 (benign) has more samples than class 0 (malignant).\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(\"\\nDataset split successfully.\")\n",
    "\n",
    "# --- 2. Preprocessing: Scale the features ---\n",
    "print(\"\\n2. Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Features scaled successfully.\")\n",
    "\n",
    "# --- 3. Modeling and Evaluation (Without Imbalance Handling) ---\n",
    "print(\"\\n3. Training models without imbalance handling...\")\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"SVM (C-Support Vector Classification)\": SVC(random_state=42),\n",
    "    \"k-NN (k-Nearest Neighbors)\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n--- {name} Results ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# --- 4. Addressing Class Imbalance ---\n",
    "print(\"\\n--- 4. Addressing Class Imbalance ---\")\n",
    "\n",
    "# Method 1: Use class_weight='balanced' for some models\n",
    "print(\"\\nUsing class_weight='balanced' parameter:\")\n",
    "balanced_models = {\n",
    "    \"Logistic Regression (Balanced)\": LogisticRegression(class_weight='balanced', random_state=42),\n",
    "    \"Random Forest (Balanced)\": RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    \"SVM (Balanced)\": SVC(class_weight='balanced', random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in balanced_models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n--- {name} Results ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Method 2: Use SMOTE (Synthetic Minority Oversampling Technique) for resampling\n",
    "# Requires installing the imbalanced-learn library: `pip install imbalanced-learn`\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "    print(\"\\nUsing SMOTE (Oversampling) with models:\")\n",
    "\n",
    "    # Create a pipeline that first oversamples and then trains the model\n",
    "    smote_models = {\n",
    "        \"Logistic Regression (SMOTE)\": ImbPipeline([('smote', SMOTE(random_state=42)), ('classifier', LogisticRegression(random_state=42))]),\n",
    "        \"Decision Tree (SMOTE)\": ImbPipeline([('smote', SMOTE(random_state=42)), ('classifier', DecisionTreeClassifier(random_state=42))]),\n",
    "        \"Random Forest (SMOTE)\": ImbPipeline([('smote', SMOTE(random_state=42)), ('classifier', RandomForestClassifier(random_state=42))])\n",
    "    }\n",
    "\n",
    "    for name, model in smote_models.items():\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        print(f\"\\n--- {name} Results ---\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nSkipping SMOTE implementation. Please install `imblearn` library to run this part.\")\n",
    "    print(\"Run: pip install imbalanced-learn\")\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")\n",
    "\n",
    "# --- 5. Analysis and Next Steps ---\n",
    "# Compare the results, especially the precision, recall, and f1-score for the minority class (class 0).\n",
    "# The class imbalance handling techniques should generally improve the recall for the minority class.\n",
    "# For example, look at the Logistic Regression results with and without 'class_weight'.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
